---
title: "Intro_to_Logreg"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Intro_to_Logreg}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r,echo = FALSE,include=FALSE}
#devtools::install_github("hobbitish1028/Logistic")
library(Logistic)
library("Rcpp")
library(glm2)
library(dplyr)
library(ggplot2)

```


## Generating binomial data
The model is Gaussian mixture model:

There are two groups, for sample j with label i (i can be 0 or 1), the distribution is 
$$X_j^i \sim N(\mu_i,\sigma^2)$$ 
The following is an toy example with 20000 samples and 100 features (we use it as training model). Among them, 10000 samples are labeled with 0 and the rest are labeled as 1. And the testing model follows the same design as training model.


```{r}

sigma<-4
set.seed(123)
n<-1e5
p<-1e2
mu1<-rnorm(p)
mu2<-rnorm(p)
X1<-matrix(mu1+rnorm(n*p,0,sigma),n,p,byrow = TRUE)
X2<-matrix(mu2+rnorm(n*p,0,sigma),n,p,byrow = TRUE)
### Train data
X<-rbind(X1,X2)
y<-rep(c(1,0),each=n)
### Test data
test_x<-rbind( matrix(mu1+rnorm(n*p,0,sigma),n,p,byrow = TRUE), 
               matrix(mu2+rnorm(n*p,0,sigma),n,p,byrow = TRUE)  )
test_y<-rep(c(1,0),each=n)

```


## How to use `LogReg` Function
Under most of the condition, we only need to input the n by p data matrix $X_{n,p}$ and binomial result $Y_n$ (whose value is 0 or 1). 



```{r}
t0<-proc.time()
fit<-Logreg(X,y)
t1<-proc.time()-t0

```

We can judge the convergence of algorithm by plotting the loss function. My function will judge by itself and stop when it converges. And the default maximal iteration number is 5000. In the application, if the results diverge according to the plot, we can tune the parameter maxit until it converges (increase the `maxit` value), which is important to achieve a great prediction. (As for the optimization detail, we use adam-like algorithm, which is quite stable and converges quickly. Different from SGD, it is less sensitive to parameter, and the default parameter can already deal with most of the condition.)

```{r}
plot(11:length(fit$loss),fit$loss[-(1:10)],main = "Convergence",xlab = "iteration",ylab = "-loglikelihood")

plot(fit$P[c(1:100,1e4+(1:100))],main="Probability (Prediction)",xlab = "sample number",ylab = "probability of group 1")
```



##How to predict with my package

We need to get the trained model(`fit`) first by using the above LogReg function. Then we just need two inputs (fit and the test matrix) to make a new prediction. The training accuracy is saved in `fit$accuracy` and the training result (the estimate of parameter) is saved in `fit$x`.

```{r}
my_prediction<-My_predict(fit,newx = X)
train_acc_mine<-mean(my_prediction == y)
test_acc_mine<-mean(my_prediction == test_y)
```



## glm
```{r}
t0<-proc.time()
dat<-as.data.frame(cbind(y,X))
fit0<-glm(y~.,data=dat,family=binomial(link=logit))
t2<-proc.time()-t0

train_acc_glm<- (mean((fit0$fitted.values[1:n]>0.5) ) + mean((fit0$fitted.values[n+(1:n)]<0.5) ) )/2

pred_glm <-cbind(rep(1,n),test_x) %*% fit0$coefficients
test_acc_glm<- mean( pred_glm*rep(c(1,-1),each=n)>0  )

```

## Comparison between three method
```{r,echo=FALSE}

class <- rep(c("train","test"),2)
accuracy<-c(train_acc_mine,test_acc_mine,train_acc_glm,test_acc_glm)
method<- rep(c("Mine","glm"),each=2)
dat<-data.frame(class,accuracy,method)

ggplot(
  dat %>%
    filter(
      method %in% c("Mine","glm")
    ) %>% group_by(class,method) %>% summarize(
      acc = accuracy
    )
)+ aes(
  x=class,
  y=acc)+
  labs(
    x="class",
    y="Accuracy"
  )+ geom_col(
    aes(fill=factor(method)),
    position='dodge'
  )+coord_cartesian( ylim = c(0.90, 0.97))+ggtitle("Comparison (three models)") +
  theme(plot.title = element_text(hjust = 0.5))

A<-matrix(c(t1[3],t2[3]),1,2)
colnames(A)<-c("My LogReg","glm")
rownames(A)<-"time (s)"
A
```


With lasso penalty and cross validation, cv.glmnet doesn't overfit, and its accuracy of test data is even higher than that of the train data. But it is very time-expensive.

My Logistic regression doesn't overfit, either. We may attribute it to the superiority of optimization algorithm (which is stable and likely to converge to global minimal rather than local minimal, thus avoiding overfitting). Besides, because it is adam-like algorithm, it converges quickly and thus needs fewer iteration. So it converges quickly.

